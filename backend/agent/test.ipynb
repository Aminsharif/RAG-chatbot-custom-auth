{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f10fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "\n",
    "# Best practice: store your credentials in environment variables\n",
    "weaviate_url = os.environ[\"WEAVIATE_URL\"]\n",
    "weaviate_api_key = os.environ[\"WEAVIATE_API_KEY\"]\n",
    "\n",
    "# Connect to Weaviate Cloud\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=weaviate_url,\n",
    "    auth_credentials=Auth.api_key(weaviate_api_key),\n",
    ")\n",
    "\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72df6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_url = os.environ.get(\"WEAVIATE_URL\")\n",
    "weaviate_api_key = os.environ.get(\"WEAVIATE_API_KEY\")\n",
    "import weaviate\n",
    "from weaviate import Client\n",
    "from weaviate.classes.init import Auth\n",
    "from weaviate.classes.query import Filter\n",
    "\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fccee906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\chatbot_project\\RAG-chatbot\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "\n",
    "def get_embeddings_model() -> Embeddings:\n",
    "    return HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = WeaviateVectorStore(\n",
    "            client=client,\n",
    "            index_name=\"pdf_index\",\n",
    "            text_key=\"text\",\n",
    "            embedding=get_embeddings_model(),\n",
    "            attributes=[\"source\", \"title\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "975655fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc5df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ret.invoke(\"what is transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "242aba38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'keywords': '', 'creator': 'LaTeX with hyperref', 'file_path': 'C:\\\\Users\\\\amins\\\\AppData\\\\Local\\\\Temp\\\\ingest_74749f49-8a8b-4c2a-bb40-c934519a72f8_1y378oaf\\\\Attention.pdf', 'trapped': '/False', 'page_label': '3', 'creationdate': datetime.datetime(2024, 4, 10, 21, 11, 43, tzinfo=datetime.timezone.utc), 'file_name': 'Attention.pdf', 'page': 2.0, 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'directory': 'C:\\\\Users\\\\amins\\\\AppData\\\\Local\\\\Temp\\\\ingest_74749f49-8a8b-4c2a-bb40-c934519a72f8_1y378oaf', 'source': 'C:/Users/amins/AppData/Local/Temp/ingest_74749f49-8a8b-4c2a-bb40-c934519a72f8_1y378oaf/Attention.pdf', 'total_pages': 15.0, 'producer': 'pdfTeX-1.40.25', 'moddate': datetime.datetime(2024, 4, 10, 21, 11, 43, tzinfo=datetime.timezone.utc), 'title': '', 'author': ''}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'keywords': '', 'creator': 'LaTeX with hyperref', 'file_path': 'C:\\\\Users\\\\amins\\\\AppData\\\\Local\\\\Temp\\\\ingest_74749f49-8a8b-4c2a-bb40-c934519a72f8_1y378oaf\\\\Attention.pdf', 'trapped': '/False', 'page_label': '5', 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'file_name': 'Attention.pdf', 'page': 4.0, 'creationdate': datetime.datetime(2024, 4, 10, 21, 11, 43, tzinfo=datetime.timezone.utc), 'directory': 'C:\\\\Users\\\\amins\\\\AppData\\\\Local\\\\Temp\\\\ingest_74749f49-8a8b-4c2a-bb40-c934519a72f8_1y378oaf', 'subject': '', 'source': 'C:/Users/amins/AppData/Local/Temp/ingest_74749f49-8a8b-4c2a-bb40-c934519a72f8_1y378oaf/Attention.pdf', 'total_pages': 15.0, 'producer': 'pdfTeX-1.40.25', 'moddate': datetime.datetime(2024, 4, 10, 21, 11, 43, tzinfo=datetime.timezone.utc), 'title': '', 'author': ''}, page_content='The Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward'),\n",
       " Document(metadata={'creator': 'LaTeX with hyperref', 'keywords': '', 'file_path': 'C:\\\\Users\\\\amins\\\\AppData\\\\Local\\\\Temp\\\\ingest_74749f49-8a8b-4c2a-bb40-c934519a72f8_1y378oaf\\\\LLM.pdf', 'trapped': '/False', 'page_label': '5', 'creationdate': datetime.datetime(2024, 4, 11, 0, 8, 1, tzinfo=datetime.timezone.utc), 'file_name': 'LLM.pdf', 'page': 4.0, 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'directory': 'C:\\\\Users\\\\amins\\\\AppData\\\\Local\\\\Temp\\\\ingest_74749f49-8a8b-4c2a-bb40-c934519a72f8_1y378oaf', 'subject': '', 'source': 'C:/Users/amins/AppData/Local/Temp/ingest_74749f49-8a8b-4c2a-bb40-c934519a72f8_1y378oaf/LLM.pdf', 'total_pages': 46.0, 'producer': 'pdfTeX-1.40.25', 'moddate': datetime.datetime(2024, 4, 11, 0, 8, 1, tzinfo=datetime.timezone.utc), 'title': 'A Comprehensive Overview of Large Language Models', 'author': 'Humza Naveed; Asad Ullah Khan; Shi Qiu; Muhammad Saqib; Saeed Anwar; Muhammad Usman; Naveed Akhtar; Nick Barnes; Ajmal Mian;'}, page_content='collected through web sources. This data contains private\\ninformation; therefore, many LLMs employ heuristics-based\\nmethods to filter information such as names, addresses, and\\nphone numbers to avoid learning personal information.\\n2.9. Architectures\\nHere we discuss the variants of the transformer architectures\\nused in LLMs. The di fference arises due to the application of\\nFigure 4: An example of attention patterns in language models, image is taken\\nfrom [93].\\nFigure 5: An example of language model training objectives, image from [93].\\nthe attention and the connection of transformer blocks. An il-\\nlustration of attention patterns of these architectures is shown\\nin Figure 4.\\nEncoder Decoder: This architecture processes inputs through\\nthe encoder and passes the intermediate representation to the\\ndecoder to generate the output. Here, the encoder sees the\\ncomplete sequence utilizing self-attention whereas the decoder\\nprocesses the sequence one after the other with implementing'),\n",
       " Document(metadata={'creator': 'LaTeX with hyperref', 'keywords': '', 'file_path': 'C:\\\\Users\\\\amins\\\\AppData\\\\Local\\\\Temp\\\\ingest_74749f49-8a8b-4c2a-bb40-c934519a72f8_1y378oaf\\\\Attention.pdf', 'trapped': '/False', 'page_label': '10', 'creationdate': datetime.datetime(2024, 4, 10, 21, 11, 43, tzinfo=datetime.timezone.utc), 'file_name': 'Attention.pdf', 'page': 9.0, 'ptex_fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'directory': 'C:\\\\Users\\\\amins\\\\AppData\\\\Local\\\\Temp\\\\ingest_74749f49-8a8b-4c2a-bb40-c934519a72f8_1y378oaf', 'source': 'C:/Users/amins/AppData/Local/Temp/ingest_74749f49-8a8b-4c2a-bb40-c934519a72f8_1y378oaf/Attention.pdf', 'total_pages': 15.0, 'producer': 'pdfTeX-1.40.25', 'moddate': datetime.datetime(2024, 4, 10, 21, 11, 43, tzinfo=datetime.timezone.utc), 'title': '', 'author': ''}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f0e994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\chatbot_project\\RAG-chatbot\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "from typing import cast, Optional, Any\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import field\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from backend import retrieval\n",
    "from backend.configuration import Configuration, IndexConfiguration\n",
    "from backend.state import InputState, State\n",
    "from backend.utils import format_docs, get_message_text, load_chat_model\n",
    "\n",
    "# Your ProdDBConfig class with user-specific configurations\n",
    "class ProdDBConfig:\n",
    "    @staticmethod\n",
    "    def _build_uri() -> str:\n",
    "        \"\"\"Production URI with pooling, SSL, timeouts.\"\"\"\n",
    "        return (\n",
    "            \"postgresql://postgres:123456@localhost:5432/langgraphrag?\"\n",
    "            \"sslmode=disable&\"\n",
    "            \"connect_timeout=10\"\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def checkpointer() -> PostgresSaver:\n",
    "        \"\"\"Get checkpointer instance.\"\"\"\n",
    "        uri = ProdDBConfig._build_uri()\n",
    "        return PostgresSaver.from_conn_string(uri)\n",
    "    \n",
    "    @staticmethod\n",
    "    def store() -> PostgresStore:\n",
    "        \"\"\"Get store instance.\"\"\"\n",
    "        uri = ProdDBConfig._build_uri()\n",
    "        return PostgresStore.from_conn_string(uri)\n",
    "\n",
    "    @staticmethod\n",
    "    @contextmanager\n",
    "    def get_store_context():\n",
    "        \"\"\"Context manager for store operations.\"\"\"\n",
    "        store = ProdDBConfig.store()\n",
    "        try:\n",
    "            yield store\n",
    "        finally:\n",
    "            if hasattr(store, 'close'):\n",
    "                store.close()\n",
    "\n",
    "# Health check helper\n",
    "def db_health_check():\n",
    "    \"\"\"Verify DB connectivity.\"\"\"\n",
    "    try:\n",
    "        with ProdDBConfig.get_store_context() as store:\n",
    "            print(\"Store connection successful\")\n",
    "        print(\"Database health check passed\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Database health check failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Define the function that calls the model\n",
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"Search the indexed documents for a query.\"\"\"\n",
    "    query: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StateGraph.compile() got an unexpected keyword argument 'config_schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 151\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# Finally, we compile it with checkpointing!\u001b[39;00m\n\u001b[32m    149\u001b[39m checkpointer = ProdDBConfig.checkpointer()\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m graph = \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mConfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m graph.name = \u001b[33m\"\u001b[39m\u001b[33mRetrievalGraph\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: StateGraph.compile() got an unexpected keyword argument 'config_schema'"
     ]
    }
   ],
   "source": [
    "\n",
    "async def generate_query(\n",
    "    state: State, *, config: RunnableConfig\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"Generate a search query based on the current state and configuration.\"\"\"\n",
    "    messages = state.messages\n",
    "    \n",
    "    # Get configuration with user_id\n",
    "    configuration = Configuration.from_runnable_config(config)\n",
    "    \n",
    "    if len(messages) == 1:\n",
    "        human_input = get_message_text(messages[-1])\n",
    "        return {\"queries\": [human_input]}\n",
    "    else:\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", configuration.query_system_prompt),\n",
    "                (\"placeholder\", \"{messages}\"),\n",
    "            ]\n",
    "        )\n",
    "        model = load_chat_model(configuration.query_model).with_structured_output(\n",
    "            SearchQuery\n",
    "        )\n",
    "\n",
    "        message_value = await prompt.ainvoke(\n",
    "            {\n",
    "                \"messages\": state.messages,\n",
    "                \"queries\": \"\\n- \".join(state.queries),\n",
    "                \"system_time\": datetime.now(tz=timezone.utc).isoformat(),\n",
    "                \"user_id\": configuration.user_id,  # Include user_id in context\n",
    "            },\n",
    "            config,\n",
    "        )\n",
    "        generated = cast(SearchQuery, await model.ainvoke(message_value, config))\n",
    "        return {\n",
    "            \"queries\": [generated.query],\n",
    "        }\n",
    "\n",
    "async def retrieve(\n",
    "    state: State, *, config: RunnableConfig\n",
    ") -> dict[str, list[Document]]:\n",
    "    \"\"\"Retrieve documents based on the latest query in the state.\"\"\"\n",
    "    # Get configuration including IndexConfiguration\n",
    "    configuration = Configuration.from_runnable_config(config)\n",
    "    user_id = configuration.user_id\n",
    "    \n",
    "    # Store user query in database for history\n",
    "    with ProdDBConfig.get_store_context() as store:\n",
    "        query_data = {\n",
    "            \"query\": state.queries[-1],\n",
    "            \"user_id\": user_id,\n",
    "            \"timestamp\": datetime.now(tz=timezone.utc).isoformat(),\n",
    "            \"metadata\": {\n",
    "                \"retriever_provider\": configuration.retriever_provider,\n",
    "                \"embedding_model\": configuration.embedding_model\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Store query with user-specific key\n",
    "        query_id = f\"query_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}\"\n",
    "        store.put(query_id, query_data)\n",
    "    \n",
    "    # Use the retriever with user-specific filtering\n",
    "    with retrieval.make_retriever(config) as retriever:\n",
    "        # If your retriever supports filtering by metadata, apply user filter\n",
    "        if hasattr(config, 'metadata_filter'):\n",
    "            config.metadata_filter = {\"user_id\": user_id}\n",
    "        \n",
    "        # Also pass user_id in search kwargs if supported\n",
    "        search_kwargs = configuration.search_kwargs.copy()\n",
    "        search_kwargs[\"metadata_filter\"] = {\"user_id\": user_id}\n",
    "        \n",
    "        response = await retriever.ainvoke(\n",
    "            state.queries[-1], \n",
    "            config={**config, \"search_kwargs\": search_kwargs}\n",
    "        )\n",
    "        return {\"retrieved_docs\": response}\n",
    "\n",
    "async def respond(\n",
    "    state: State, *, config: RunnableConfig\n",
    ") -> dict[str, list[BaseMessage]]:\n",
    "    \"\"\"Call the LLM powering our \"agent\".\"\"\"\n",
    "    configuration = Configuration.from_runnable_config(config)\n",
    "    user_id = configuration.user_id\n",
    "    \n",
    "    # Store conversation in the database with user filtering\n",
    "    with ProdDBConfig.get_store_context() as store:\n",
    "        # Store conversation metadata with user_id\n",
    "        conversation_data = {\n",
    "            \"user_id\": user_id,\n",
    "            \"messages\": [msg.dict() for msg in state.messages],\n",
    "            \"queries\": state.queries,\n",
    "            \"retrieved_docs\": [doc.dict() for doc in state.retrieved_docs],\n",
    "            \"timestamp\": datetime.now(tz=timezone.utc).isoformat(),\n",
    "            \"configuration\": {\n",
    "                \"retriever_provider\": configuration.retriever_provider,\n",
    "                \"embedding_model\": configuration.embedding_model\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Store with user-specific key\n",
    "        conversation_id = f\"conversation_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        store.put(conversation_id, conversation_data)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", configuration.response_system_prompt),\n",
    "            (\"placeholder\", \"{messages}\"),\n",
    "        ]\n",
    "    )\n",
    "    model = load_chat_model(configuration.response_model)\n",
    "\n",
    "    retrieved_docs = format_docs(state.retrieved_docs)\n",
    "    message_value = await prompt.ainvoke(\n",
    "        {\n",
    "            \"messages\": state.messages,\n",
    "            \"retrieved_docs\": retrieved_docs,\n",
    "            \"system_time\": datetime.now(tz=timezone.utc).isoformat(),\n",
    "            \"user_id\": user_id,  # Include user_id in prompt context\n",
    "        },\n",
    "        config,\n",
    "    )\n",
    "    response = await model.ainvoke(message_value, config)\n",
    "    \n",
    "    # Store the response separately\n",
    "    with ProdDBConfig.get_store_context() as store:\n",
    "        response_data = {\n",
    "            \"user_id\": user_id,\n",
    "            \"query\": state.queries[-1] if state.queries else \"\",\n",
    "            \"response\": response.content if hasattr(response, 'content') else str(response),\n",
    "            \"timestamp\": datetime.now(tz=timezone.utc).isoformat(),\n",
    "            \"conversation_id\": conversation_id\n",
    "        }\n",
    "        response_key = f\"response_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}\"\n",
    "        store.put(response_key, response_data)\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define a new graph\n",
    "builder = StateGraph(State, input_schema=InputState, context_schema=Configuration)\n",
    "\n",
    "builder.add_node(generate_query)\n",
    "builder.add_node(retrieve)\n",
    "builder.add_node(respond)\n",
    "builder.add_edge(\"__start__\", \"generate_query\")\n",
    "builder.add_edge(\"generate_query\", \"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"respond\")\n",
    "\n",
    "# Finally, we compile it with checkpointing!\n",
    "checkpointer = ProdDBConfig.checkpointer()\n",
    "\n",
    "graph = builder.compile(\n",
    "    checkpointer=checkpointer,\n",
    "    interrupt_before=[],\n",
    "    interrupt_after=[],\n",
    ")\n",
    "graph.name = \"RetrievalGraph\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enhanced functions for user-specific operations\n",
    "def load_user_conversation(thread_id: str, user_id: str, config: Optional[dict] = None):\n",
    "    \"\"\"Load a conversation from checkpoint by thread ID for specific user.\"\"\"\n",
    "    configurable = {\n",
    "        \"thread_id\": thread_id,\n",
    "        \"user_id\": user_id\n",
    "    }\n",
    "    if config:\n",
    "        configurable.update(config.get(\"configurable\", {}))\n",
    "    \n",
    "    return graph.get_state({\"configurable\": configurable})\n",
    "\n",
    "def list_user_conversations(user_id: str, limit: int = 100):\n",
    "    \"\"\"List all stored conversation thread IDs for a specific user.\"\"\"\n",
    "    with ProdDBConfig.checkpointer() as cp:\n",
    "        # This will need adjustment based on your PostgresSaver implementation\n",
    "        # You might need to implement custom filtering\n",
    "        all_conversations = cp.list({\"configurable\": {}}, limit=limit)\n",
    "        \n",
    "        # Filter by user_id if stored in checkpoint metadata\n",
    "        user_conversations = []\n",
    "        for conv in all_conversations:\n",
    "            if hasattr(conv, 'metadata') and conv.metadata.get('user_id') == user_id:\n",
    "                user_conversations.append(conv)\n",
    "        \n",
    "        return user_conversations\n",
    "\n",
    "def get_user_conversation_history(user_id: str, limit: int = 50):\n",
    "    \"\"\"Get conversation history from store for a specific user.\"\"\"\n",
    "    with ProdDBConfig.get_store_context() as store:\n",
    "        # This assumes your store supports scanning or querying by prefix\n",
    "        # Adjust based on your PostgresStore implementation\n",
    "        conversations = []\n",
    "        \n",
    "        # Try to get conversations by pattern\n",
    "        # Note: Actual implementation depends on your store's query capabilities\n",
    "        try:\n",
    "            # If store has scan or query capabilities\n",
    "            if hasattr(store, 'scan'):\n",
    "                for key, value in store.scan():\n",
    "                    if key.startswith(f\"conversation_{user_id}_\"):\n",
    "                        conversations.append({\n",
    "                            \"id\": key,\n",
    "                            \"data\": value,\n",
    "                            \"timestamp\": value.get(\"timestamp\", \"\")\n",
    "                        })\n",
    "            \n",
    "            # Sort by timestamp\n",
    "            conversations.sort(key=lambda x: x.get(\"timestamp\", \"\"), reverse=True)\n",
    "            \n",
    "            return conversations[:limit]\n",
    "        except:\n",
    "            # Fallback - store implementation might differ\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03959670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Health check\n",
    "    if db_health_check():\n",
    "        print(\"Database is ready\")\n",
    "        \n",
    "        # Example: Invoke the graph with user-specific configuration\n",
    "        user_id = \"user_12345\"  # Or use UUID\n",
    "        config = {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": f\"{user_id}_session_1\",\n",
    "                \"user_id\": user_id,\n",
    "                \"retriever_provider\": \"weaviate\",\n",
    "                \"embedding_model\": \"all-MiniLM-L6-v2\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # This will automatically filter and store data for this user\n",
    "        result = graph.invoke(\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]},\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        print(\"Response:\", result[\"messages\"][-1].content)\n",
    "        \n",
    "        # Get user's conversation history\n",
    "        history = get_user_conversation_history(user_id, limit=10)\n",
    "        print(f\"\\nUser {user_id} has {len(history)} conversations\")\n",
    "        \n",
    "        # Continue conversation with same user\n",
    "        result2 = graph.invoke(\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me more about that\"}]},\n",
    "            config=config  # Same thread_id and user_id will load previous state\n",
    "        )\n",
    "        \n",
    "        print(\"\\nFollow-up response:\", result2[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5512e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main entrypoint for the conversational retrieval graph with user memory.\n",
    "\n",
    "This module defines the core structure and functionality of the conversational\n",
    "retrieval graph with stateful user preferences via langgraph.json store.\n",
    "Supports cross-thread memory (user prefs) + thread-scoped history.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any, Annotated, TypedDict, cast, Sequence\n",
    "from operator import add\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel as PydanticBaseModel\n",
    "\n",
    "from backend import retrieval\n",
    "from backend.configuration import Configuration\n",
    "from backend.state import InputState, State  # Assume State updated below\n",
    "from backend.utils import format_docs, get_message_text, load_chat_model\n",
    "\n",
    "# User preference schema for tools\n",
    "class UserInfo(PydanticBaseModel):\n",
    "    \"\"\"User preferences schema.\"\"\"\n",
    "    name: str = Field(..., description=\"User's full name\")\n",
    "    favorite_topics: list[str] = Field(default_factory=list, description=\"Favorite topics\")\n",
    "    preferred_format: str = Field(default=\"detailed\", description=\"Response style: brief/detailed\")\n",
    "\n",
    "# Memory tools - access store via config[\"store\"] (langgraph.json injection)\n",
    "@tool\n",
    "async def get_user_prefs(runtime: Any) -> str:\n",
    "    \"\"\"Get stored preferences for current user.\"\"\"\n",
    "    store = runtime.config.get(\"store\")\n",
    "    if not store:\n",
    "        return \"No store available.\"\n",
    "    user_id = runtime.config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"users\", user_id)\n",
    "    item = await store.aget(namespace, \"profile\")\n",
    "    if item and item.value:\n",
    "        prefs = item.value\n",
    "        return f\"Name: {prefs.get('name', 'Unknown')}\\nTopics: {prefs.get('favorite_topics', [])}\\nFormat: {prefs.get('preferred_format', 'detailed')}\"\n",
    "    return \"No preferences stored yet.\"\n",
    "\n",
    "@tool\n",
    "async def save_user_prefs(info: UserInfo, runtime: Any) -> str:\n",
    "    \"\"\"Save/update user preferences.\"\"\"\n",
    "    store = runtime.config.get(\"store\")\n",
    "    if not store:\n",
    "        return \"No store available.\"\n",
    "    user_id = runtime.config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"users\", user_id)\n",
    "    await store.aput(namespace, \"profile\", info.model_dump())\n",
    "    return \"Preferences saved!\"\n",
    "\n",
    "class SearchQuery(PydanticBaseModel):\n",
    "    \"\"\"Search the indexed documents for a query.\"\"\"\n",
    "    query: str\n",
    "\n",
    "async def generate_query(\n",
    "    state: State, *, config: RunnableConfig\n",
    ") -> dict[str, list[str]]:\n",
    "    \"\"\"Generate a search query based on the current state and configuration.\"\"\"\n",
    "    messages = state.messages\n",
    "    if len(messages) == 1:\n",
    "        human_input = get_message_text(messages[-1])\n",
    "        return {\"queries\": [human_input]}\n",
    "    else:\n",
    "        configuration = Configuration.from_runnable_config(config)\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", configuration.query_system_prompt),\n",
    "            (\"placeholder\", \"{messages}\"),\n",
    "        ])\n",
    "        model = load_chat_model(configuration.query_model).with_structured_output(SearchQuery)\n",
    "\n",
    "        message_value = await prompt.ainvoke({\n",
    "            \"messages\": state.messages,\n",
    "            \"queries\": \"\\n- \".join(state.queries),\n",
    "            \"system_time\": datetime.now(tz=timezone.utc).isoformat(),\n",
    "        }, config)\n",
    "        generated = cast(SearchQuery, await model.ainvoke(message_value, config))\n",
    "        return {\"queries\": [generated.query]}\n",
    "\n",
    "async def retrieve(\n",
    "    state: State, *, config: RunnableConfig\n",
    ") -> dict[str, list[Document]]:\n",
    "    \"\"\"Retrieve documents based on the latest query in the state.\"\"\"\n",
    "    with retrieval.make_retriever(config) as retriever:\n",
    "        response = await retriever.ainvoke(state.queries[-1], config)\n",
    "        return {\"retrieved_docs\": response}\n",
    "\n",
    "async def respond(\n",
    "    state: State, *, config: RunnableConfig\n",
    ") -> dict[str, Sequence[BaseMessage]]:\n",
    "    \"\"\"Enhanced respond with user memory injection + tools.\"\"\"\n",
    "    store = config.get(\"store\")  # langgraph.json injects AsyncPostgresStore\n",
    "    configuration = Configuration.from_runnable_config(config)\n",
    "    \n",
    "    # Load user prefs from store\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"users\", user_id)\n",
    "    profile_item = await store.aget(namespace, \"profile\") if store else None\n",
    "    \n",
    "    user_context = \"\"\n",
    "    if profile_item and profile_item.value:\n",
    "        prefs = profile_item.value\n",
    "        user_context = (\n",
    "            f\"User preferences: Name={prefs.get('name', 'Unknown')}, \"\n",
    "            f\"Topics={prefs.get('favorite_topics', [])}, \"\n",
    "            f\"Format={prefs.get('preferred_format', 'detailed')}.\\n\"\n",
    "            f\"Adapt responses accordingly.\"\n",
    "        )\n",
    "    \n",
    "    # Enhanced prompt with user context\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"{configuration.response_system_prompt}\\n\\n{user_context}\"),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ])\n",
    "    \n",
    "    model = load_chat_model(configuration.response_model)\n",
    "    # Bind memory tools\n",
    "    model_with_tools = model.bind_tools([get_user_prefs, save_user_prefs])\n",
    "    \n",
    "    retrieved_docs = format_docs(state.retrieved_docs)\n",
    "    message_value = await prompt.ainvoke({\n",
    "        \"messages\": state.messages,\n",
    "        \"retrieved_docs\": retrieved_docs,\n",
    "        \"system_time\": datetime.now(tz=timezone.utc).isoformat(),\n",
    "    }, config)\n",
    "    \n",
    "    # Invoke with store access for tools\n",
    "    response = await model_with_tools.ainvoke(message_value, config)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Updated State (add annotations if needed)\n",
    "class RetrievalState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    queries: Annotated[list[str], add]\n",
    "    retrieved_docs: list[Document]\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(RetrievalState, input_schema=InputState)  # Use your InputState\n",
    "builder.add_node(\"generate_query\", generate_query)\n",
    "builder.add_node(\"retrieve\", retrieve)\n",
    "builder.add_node(\"respond\", respond)\n",
    "\n",
    "builder.add_edge(\"__start__\", \"generate_query\")\n",
    "builder.add_edge(\"generate_query\", \"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"respond\")\n",
    "builder.add_edge(\"respond\", END)\n",
    "\n",
    "# NO manual store/checkpointer - langgraph.json handles it!\n",
    "graph = builder.compile(\n",
    "    interrupt_before=[],\n",
    "    interrupt_after=[],\n",
    ")\n",
    "graph.name = \"RetrievalGraph\"\n",
    "\n",
    "# Example usage function\n",
    "async def chat_example():\n",
    "    config = {\"configurable\": {\"thread_id\": \"chat1\", \"user_id\": \"alice123\"}}\n",
    "    \n",
    "    # Session 1: Learns prefs + retrieves\n",
    "    result1 = await graph.ainvoke({\n",
    "        \"messages\": [HumanMessage(content=\"Hi I'm Alice, love hiking/tech. What's RAG?\")]\n",
    "    }, config)\n",
    "    print(\"Response 1:\", result1[\"messages\"][-1].content)\n",
    "    \n",
    "    # New thread: Uses prefs in retrieval context!\n",
    "    config2 = {\"configurable\": {\"thread_id\": \"chat2\", \"user_id\": \"alice123\"}}\n",
    "    result2 = await graph.ainvoke({\n",
    "        \"messages\": [HumanMessage(content=\"More on RAG, make it brief\")]\n",
    "    }, config2)\n",
    "    print(\"Response 2:\", result2[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc744c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
